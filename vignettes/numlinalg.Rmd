---
title: "numlinalg"
author: "Ruifeng Chen"
date: "2018/10/23"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Regression Methods**

Load library "datasets" which contains longley we will analyze later.

Load library "gmp" that contains function "as.bigq" which encodes rationals encoded as ratios of arbitrary large integers.
```{r}
library(datasets)
library(gmp)
```

*(a)*
Function "regression.exact" will compute the exact coefficients of regression model. Arguments of the function are a vector y and a matrix X.
```{r}
regression.exact <- function(y,X){
  y <- as.bigq(y)
  X <- as.bigq(X)
  beta <- as.double(solve(t(X)%*%X,t(X)%*%y))
  return(beta)
}
```

*(b)*
Function "Cholesky" uses the Cholesky factorization of the cross product matrix to compute the coefficients of a least squares fit of a vector y to the columns of a matrix X.

There're three arguments of function "Cholesky", a vector y, a matrix X and choice center with default FALSE. If center is TRUE then X should contain only non-constant columns and it will mean center the columns before forming the Cholesky factorization.
```{r}
Cholesky <- function(y,X,center = FALSE){
  if(center){
    X <- sweep(X,2,colMeans(X))[,-1]
    y <- y-mean(y)
    U <- chol(t(X) %*% X)
    UXY <- forwardsolve(t(U), t(X)%*%y)
    beta <- as.double(backsolve(U,UXY))
  } else{
    U <- chol(t(X) %*% X)
    UXY <- forwardsolve(t(U), t(X)%*%y)
    beta <- as.double(backsolve(U,UXY))
  }
  return(beta)
}
```

*(c)*
We use data set longley to compare the accuracy of coefficient estimates obtained by the QR and Cholesky approaches with and without centering.

Without centering, we can see that QR factorization is better than Cholesky approache since all coefficients got by QR factorization are closer to the exact coefficients comparing to Cholesky factorization.
```{r}
y <- as.vector(longley[,7])
X <- cbind(1,as.matrix(longley[,1:6]))

Cholesky(y,X)-regression.exact(y,X)
coef(lm.fit(X,y))-regression.exact(y,X)
```

It is helpful to mean center the data to apply Cholesky factorization, but the result is still not as good as QR factorization.
```{r}
X.centered <- sweep(X,2,colMeans(X))[,-1]
y.centered <- y - mean(y)

Cholesky(y,X,center = T)-regression.exact(y.centered,X.centered)
coef(lm.fit(X.centered,y.centered))-regression.exact(y.centered,X.centered)
```

**Sparse Matrix Computations**
We want to calculate the log-likelihood for an observed vector y, which is:
$$-\frac{1}{2}log(det(C))-\frac{1}{2}y^TC^{-1}y$$
where C only has elements 1 on diagonal, and a scalar a on sub-diagonal and upper-diagonal.

*(a)*
Function "log.likelihood1" will compute the log likelihood for a vector y and a scalar a using dense matrix methods.

First of all, it will apply Cholesky factorization to matrix C, which is easier for us to get the matrix inverse. In addition, this can avoid the computation of log of determint which is very hard to calculate. Instead, it will decomposite the matrix to triangular, then the determinant is much easier to conduct.
```{r}
log.likelihood1 <- function(y, scalar = a){
  C <- diag(1,length(y))
  C[ cbind(2 : nrow(C), 1 : (nrow(C) - 1)) ] <- a
  C[ cbind(1 : (nrow(C)-1), 2 : nrow(C)) ] <- a
  U <- chol(C)
  UY <- forwardsolve(t(U), y)
  loglikelihood <- -1/2*2*sum(log(diag(U)))-1/2*t(UY)%*%UY
  return(loglikelihood)
}
```

*(b)*
Function "log.likelihood2" will use the sparse matrix methods provided by the "Matrix" package to take advantage of the sparseness of the covariance matrix. We use "bandSparse" to get the matrix C. It avoid the function "cbind" which is pretty slow.
```{r}
library(Matrix)

log.likelihood2 <- function(y, scalar = a){
  diags <-list(rep(1,length(y)),rep(a,length(y)))
  C <- bandSparse(length(y), k = c(0,1), diag = diags, symm = TRUE)
  U <- chol(C)
  UY <- forwardsolve(t(U), y)
  loglikelihood <- -1/2*2*sum(log(diag(U)))-1/2*t(UY)%*%UY
  return(loglikelihood)
}
```

*(c)*
We compare the performance of your two functions on data vectors of two different lengths, one is with length 100, the other one is with length 1000. Each element is generated by standard normal. The scalar is set to 0.5.

We can see in the first case, the sparse matrix approach is as fast as the dense matrix approach. However, with large length of vector, the sparse matrix approach is much faster than the dense matrix approach.
```{r}
y1 <- rnorm(100,0,1)
y2 <- rnorm(1000,0,1)
a <- 0.5

system.time(log.likelihood1(y1,a))
system.time(log.likelihood2(y1,a))

system.time(log.likelihood1(y2,a))
system.time(log.likelihood2(y2,a))
```

